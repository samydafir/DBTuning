\documentclass[11pt]{scrartcl}

\usepackage{url,float}

\title{
  \textbf{\large Database Tuning -- Assignment 1}\\
  Uploading Data to the Database
}

\author{
 A2\\
 \large Lastname1 Firstname1, StudentID1 \\
 \large Lastname2 Firstname2, StudentID2 \\
 \large Sch\"orgnhofer Kevin, 1421082
}

\begin{document}

\maketitle

\subsection*{Straightforward Implementation}

  \paragraph{Implementation}

  For the Straightforward Implementation we create a single SQL-INSERT command for every line of the given input file. We send the current command to the database server, before we start creating the SQL-INSERT command for the next line of the input file.

{\small
\begin{verbatim}
    INSERT INTO auth VALUES ('Jurgen Annevelink','books/acm/kim95/AnnevelinkACFHK95');
    INSERT INTO auth VALUES ('Rafiul Ahad','books/acm/kim95/AnnevelinkACFHK95');
    INSERT INTO auth VALUES ('Amelia Carlson','books/acm/kim95/AnnevelinkACFHK95');
    etc.
\end{verbatim}
}

  \subsection*{Efficient Approach 1: Single Query}

  \paragraph{Implementation}

  For this approach we build one huge SQL-INSERT command which contains all the values of the input file.

{\small
\begin{verbatim}
    INSERT  INTO auth VALUES
    ('Jurgen Annevelink','books/acm/kim95/AnnevelinkACFHK95'),
    ('Rafiul Ahad','books/acm/kim95/AnnevelinkACFHK95'),
    ('Amelia Carlson','books/acm/kim95/AnnevelinkACFHK95'),
    ...
    ('Klaus Richter','series/vdi/SchenkR07');
\end{verbatim}
}

  \paragraph{Why is this approach efficient?}

  There are two main reasons, why this approach is faster than the straightforward implementation:
  \begin{enumerate}
  	\item Compared to the straightforward approach we have only about 60\% of the amount of data which has to be sent via the network to the database server. In the straightforward approach we have for every single input line an extra "INSERT INTO auth VALUES"-statement, which is about 40\% of the whole INSERT-command (INSERT-command = INSERT-statement + values). In this approach we need the INSERT-statement only once, causing the size of the transferred data to shrink to about 60\%. Less data needs less time to be sent.
    \item For the straightforward approach we send about 3 million sql commands via the network to the database server, whereas for this approach we need only one. This cuts down the overhead for sending the sql command(s) vastly. Arrived at the database server, this time the database has to parse, optimize and create/access paths to the data only once (and not 3 million times), which again saves a lot of time.
  \end{enumerate}
This approach was already known by us and the reasons why this approach is faster are pretty obvious (which is the reason why there are no references for this approach).

  \paragraph{Tuning principle}

  "Start-up Costs Are High; Running Costs Are Low":
  \\
  For this approach we tried to care for the \textbf{network latency} and cut down the \textbf{query overhead} with using only one single query.

  \subsection*{Efficient Approach 2: COPY FROM statement}

\paragraph{Implementation}

For the second improved approach we wanted to use the "COPY FROM"-statement to import the data directly from a file into the database. Unfortunately this statement requires superuser rights which we do not have. In fact we were not able to use this approach, which is the reason why we searched for a third approach.

{\small
\begin{verbatim}
  COPY  auth FROM '/home/stud3/%user%/Documents/dbtuning/blatt01/auth.tsv' DELIMITER '\tab';
\end{verbatim}
}

\paragraph{Why is this approach efficient?}

As already mentioned we were not able to actually test this approach. But the following arguments lead to the assumption that it should be faster than the straightforward approach:
\begin{itemize}
\item The size of the data which has to be transferred via the network is pretty small. You have only the COPY-statement, the path to the document and the option "DELIMITER". Less data needs less time to be sent.
\item There is only one sql command sent, and not about 3 million, which causes much less overhead and network traffic at all. Additionally the database has to parse, optimize and create/access paths to the data only once.
\item In the straightforward approach the computer which executes the import program has to calculate all the 3 million SQL-commands, in this approach the executing computer has to calculate nearly nothing. All the work does the database server, which has probably more computing resources.
\item The COPY-statement is a statement which was made for fast imports (and exports) from files (or to files), so it is probably faster than a separate program which first reads data from a file and then sends it to the database as an INSERT-statement.

Important: Cite the references that you used to answer this
question, for example, with footnotes\footnote{PostgreSQL 9.2
  Documentation, Chapter VI.I SQL Commands,
  \url{https://www.postgresql.org/docs/9.2/static/sql-copy.html}}.

\paragraph{Tuning principle}
"Start-up Costs Are High; Running Costs Are Low":
\\
Like for our first improved approach we have here again \textbf{less network traffic} and a small \textbf{query overhead}.
\\
"Render on the Server What Is Due on the Server":
The combination of the import instruction and the values is not created on client side, but on server side.

    \subsection*{Efficient Approach 2: (NAME)}

  \paragraph{Implementation}

  Describe in a few lines how this approach works and show the query
  that you use. You may also show small (!) code snippets if you think
  they help the understanding.

{\small
\begin{verbatim}
    MY SQL QUERY ...
\end{verbatim}
}

  \paragraph{Why is this approach efficient?}

  Explain, why this approach is more efficient than the
  straightforward approach. Where does the system save the time? Be
  clear and precise!

  Important: Cite the references that you used to answer this
  question, for example, with footnotes\footnote{PostgreSQL 9.0
    Documentation, Chapter 3.5,
    \url{http://www.postgresql.org/docs/9.0/static/tutorial-window.html}}.

  \paragraph{Tuning principle}

  Which tuning principle did you apply? Pick the one that describes
  this approach best (``thing globally, fix locally'' is too general).

  \subsection*{Runtime Experiment}

  \begin{table}[H]
  \begin{tabular}{l|r}
    Approach & Runtime [sec] \\
    \hline
    Straightforward & ... \\
    Approach 1 (NAME) & ... \\
    Approach 2 (NAME) & ...
  \end{tabular}
  \end{table}

  \bigskip

  \noindent Notes:
  \begin{itemize}
  \item For the straightforward approach you are allowed to import
    only a subset of the tuples (e.g., 10000 tuples) and estimate the
    overall runtime. The timings for all other approaches should be
    real measurements over the whole dataset.
  \item Specify the setting of the experiment, i.e., where is the
    database server (local machine, database server at the
    department), where is the client (wired/wireless network of the
    department)?
\end{itemize}

  \subsection*{Time Spent on this Assignment}

  Time in hours per person: {\bf XXX}

\end{document}
