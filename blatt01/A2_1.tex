\documentclass[11pt]{scrartcl}

\usepackage{url,float}

\title{
  \textbf{\large Database Tuning -- Assignment 1}\\
  Uploading Data to the Database
}

\author{
 Group Name (e.g. A1, B5, B3)\\
 \large Lastname1 Firstname1, StudentID1 \\
 \large Lastname2 Firstname2, StudentID2 \\
 \large Sch\"orgnhofer Kevin, 1421082
}

\begin{document}

\maketitle

\subsection*{Straightforward Implementation}

  \paragraph{Implementation}

  For the Straightforward Implementation we create a SQL-INSERT command for every single line of the given input file. We send the current command to the database server, before we start creating the SQL-INSERT command for the next line of the input file.

{\small
\begin{verbatim}
    INSERT INTO auth VALUES ('Jurgen Annevelink','books/acm/kim95/AnnevelinkACFHK95');
    INSERT INTO auth VALUES ('Rafiul Ahad','books/acm/kim95/AnnevelinkACFHK95');
    INSERT INTO auth VALUES ('Amelia Carlson','books/acm/kim95/AnnevelinkACFHK95');
    etc.
\end{verbatim}
}

  \subsection*{Efficient Approach 1: (NAME)}

  \paragraph{Implementation}

  For this approach we build one huge SQL-INSERT command which contains all the values of the input file.

{\small
\begin{verbatim}
    INSERT  INTO auth VALUES
    ('Jurgen Annevelink','books/acm/kim95/AnnevelinkACFHK95'),
    ('Rafiul Ahad','books/acm/kim95/AnnevelinkACFHK95'),
    ('Amelia Carlson','books/acm/kim95/AnnevelinkACFHK95'),
    ...
    ('Klaus Richter','series/vdi/SchenkR07');
\end{verbatim}
}

  \paragraph{Why is this approach efficient?}

  There are two main reasons, why this approach is faster than the straightforward implementation:
  \begin{enumerate}
  	\item Compared to the straightforward approach we have only about 60\% of the amount of data which has to be sent via the network to the database server. In the straightforward approach we have for every single input line an extra "INSERT INTO auth VALUES"-statement, which is about 40\% of the whole INSERT-command (INSERT-command = INSERT-statement + values). In this approach we need the INSERT-statement only once, causing the size of the transferred data to shrink to about 60\%. Less data needs less time to be sent.
    \item For the straightforward approach we send about 3 million sql commands via the network to the database server, whereas for this approach we need again only one. This cuts down the overhead for sending the sql command(s) vastly. Arrived at the database server, this time the database has to parse, optimize and create/access paths to the data only once (and not 3 million times), which again saves a lot of time.
  \end{enumerate}
This approach was already known by us (so no references for this one).

  \paragraph{Tuning principle}

  "Start-up Costs Are High; Running Costs Are Low":
  \\
  For this approach we tried to care for the \textbf{network latency} and cut down the \textbf{query overhead} with using only one single query.

    \subsection*{Efficient Approach 2: (NAME)}

  \paragraph{Implementation}

  Describe in a few lines how this approach works and show the query
  that you use. You may also show small (!) code snippets if you think
  they help the understanding.

{\small
\begin{verbatim}
    MY SQL QUERY ...
\end{verbatim}
}

  \paragraph{Why is this approach efficient?}

  Explain, why this approach is more efficient than the
  straightforward approach. Where does the system save the time? Be
  clear and precise!

  Important: Cite the references that you used to answer this
  question, for example, with footnotes\footnote{PostgreSQL 9.0
    Documentation, Chapter 3.5,
    \url{http://www.postgresql.org/docs/9.0/static/tutorial-window.html}}.

  \paragraph{Tuning principle}

  Which tuning principle did you apply? Pick the one that describes
  this approach best (``thing globally, fix locally'' is too general).

  \subsection*{Runtime Experiment}

  \begin{table}[H]
  \begin{tabular}{l|r}
    Approach & Runtime [sec] \\
    \hline
    Straightforward & ... \\
    Approach 1 (NAME) & ... \\
    Approach 2 (NAME) & ...
  \end{tabular}
  \end{table}

  \bigskip

  \noindent Notes:
  \begin{itemize}
  \item For the straightforward approach you are allowed to import
    only a subset of the tuples (e.g., 10000 tuples) and estimate the
    overall runtime. The timings for all other approaches should be
    real measurements over the whole dataset.
  \item Specify the setting of the experiment, i.e., where is the
    database server (local machine, database server at the
    department), where is the client (wired/wireless network of the
    department)?
\end{itemize}

  \subsection*{Time Spent on this Assignment}

  Time in hours per person: {\bf XXX}

\end{document}
